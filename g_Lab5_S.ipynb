{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 5: Was Our Model Any Good? - Validation & Metrics\n",
    "\n",
    "**Student Name:** [Your Name]\n",
    "**Roll Number:** [Your Roll Number]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìñ Story Point\n",
    "Our Decision Tree model reported 100% accuracy. That seems too good to be true. Is accuracy the only thing that matters? What if the cost of a wrong prediction is very high? We need to go beyond simple accuracy and use more robust evaluation techniques and metrics to truly understand our model's performance and trustworthiness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üéØ Objective\n",
    "Evaluate the classification model from Lab 4 using a Confusion Matrix, Precision, Recall, F1-Score, and K-Fold Cross-Validation to get a more complete picture of its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ In-Class Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1: Setup and Model Training\n",
    "This first step is a recap of Lab 4. Load the data, create the features (`X`) and target (`y`), split the data, and train a `DecisionTreeClassifier` with `random_state=42`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2: Generate and Visualize a Confusion Matrix\n",
    "Make predictions on the test set. Then, import `confusion_matrix` and `ConfusionMatrixDisplay` from `sklearn.metrics`. Generate and plot the confusion matrix to see the breakdown of correct and incorrect predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3: Calculate Precision, Recall, and F1-Score\n",
    "Accuracy can be misleading. Import `precision_score`, `recall_score`, and `f1_score`. Calculate and print these metrics for your model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4: Interpretation\n",
    "Imagine our model is used to decide whether to place a large bet on a match being 'High-Scoring'. \n",
    "- In this scenario, would you want to prioritize **Precision** or **Recall**? \n",
    "- Explain your choice in 1-2 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*(Double-click here to write your answer)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### deliverables\n",
    "A notebook containing a confusion matrix visualization, a table or printout of classification metrics (Precision, Recall, F1-Score), and your interpretation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üè° Home Task Extension\n",
    "A single train-test split can be lucky or unlucky. A much more robust way to evaluate a model is with K-Fold Cross-Validation, which performs multiple splits and averages the results.\n",
    "\n",
    "**Your Task:**\n",
    "1.  Import `cross_val_score` from `sklearn.model_selection`.\n",
    "2.  Use `cross_val_score` to evaluate your `DecisionTreeClassifier` on the **entire dataset** (`X`, `y`). Set the number of folds `cv=5`.\n",
    "3.  The function will return an array of scores, one for each fold. Print this array.\n",
    "4.  Calculate and print the **mean** and **standard deviation** of these scores. Explain what this tells you about the model's stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}