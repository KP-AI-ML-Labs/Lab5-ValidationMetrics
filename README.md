# Lab 5: Was Our Model Any Good? ‚Äì Validation & Metrics

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](
https://github.com/KP-AI-ML-Labs/Lab5-ValidationMetrics/blob/main/g_Lab5_S.ipynb)

---

## üìñ Story Point  
Accuracy isn‚Äôt the full story. Let‚Äôs dive deeper into model evaluation to avoid costly mistakes.  

---

## üéì Mapping to Lecture  
- **Topic:** Validation, Bias-Variance Tradeoff, Performance Metrics  
- **Python Libraries:** `scikit-learn`, `matplotlib`, `seaborn`  
- **Category:** Model Evaluation  

---

## üß™ In-Class Practical  

**Objective:**  
Properly evaluate the classification model from Lab 4.  

**Tasks:**  
1. Generate a **Confusion Matrix** for the Decision Tree‚Äôs predictions  
2. Calculate **Precision, Recall, F1-Score**  
3. Explain scenarios where **precision > recall** and vice versa  

---

## üè° Home Task  
- Implement **5-Fold Cross-Validation** on the Decision Tree  
- Report mean & std. deviation of accuracy across folds  
- Explain why CV is more robust than a single split  

---

## ‚úÖ Deliverable  
A confusion matrix visualization & table of Precision, Recall, and F1-Score.  

---

## ‚öô Requirements  
Install required libraries:  
`!pip install -r requirements.txt
`
